{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from doepy import build\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import random\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IISEPaper import categorise\n",
    "from IISEPaper import get_labeled_index\n",
    "from IISEPaper import get_feature_label\n",
    "from IISEPaper import get_labeled_set\n",
    "from IISEPaper import get_training_Set\n",
    "from IISEPaper import get_unlabeled_set\n",
    "from IISEPaper import c_prediction\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from IISEPaper import predicted_region_divide\n",
    "from IISEPaper import get_labeled_feasible_sample\n",
    "from IISEPaper import get_feasible_rep\n",
    "from IISEPaper import get_div_term\n",
    "from IISEPaper import find_indices\n",
    "from IISEPaper import get_select_index\n",
    "from IISEPaper import GP_predict\n",
    "from IISEPaper import labeled_fea_sample\n",
    "from IISEPaper import remove_label\n",
    "from IISEPaper import get_index_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianProcessClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset1101.csv')\n",
    "ds,X_feature,y_ground,y_r = get_feature_label(data)\n",
    "full_index = list(range(y_ground.shape[0]))\n",
    "index_file = np.load('data1_10_index.npy',allow_pickle = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_index_div(div, unlabeled_index):\n",
    "    index=[]\n",
    "    max_div = max(div)\n",
    "    temp = list(div).index(max_div)\n",
    "    index.append(unlabeled_index[temp])\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0 finished\n",
      "Round 1 finished\n",
      "Round 2 finished\n",
      "Round 3 finished\n",
      "Round 4 finished\n",
      "Round 5 finished\n",
      "Round 6 finished\n",
      "Round 7 finished\n",
      "Round 8 finished\n",
      "Round 9 finished\n",
      "Round 10 finished\n",
      "Round 11 finished\n",
      "Round 12 finished\n",
      "Round 13 finished\n",
      "Round 14 finished\n",
      "Round 15 finished\n",
      "Round 16 finished\n",
      "Round 17 finished\n",
      "Round 18 finished\n",
      "Round 19 finished\n",
      "Round 20 finished\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "result=[]\n",
    "for n in range(len(index_file)):\n",
    "    labeled_index= list(index_file[n])\n",
    "    full_index = list(range(y_ground.shape[0]))\n",
    "    x_labeled = X_feature[labeled_index]\n",
    "    y_labeled = y_ground[labeled_index]\n",
    "    y_r_labeled = y_r[labeled_index]\n",
    "    unlabeled_index = remove_label(full_index,labeled_index)\n",
    "    unlabeled_feature = X_feature[unlabeled_index]\n",
    "    r_selected = y_r[labeled_index]\n",
    "    labeled_feasible_sample, labeled_infeasible_sample,labeled_r = labeled_fea_sample(x_labeled,y_labeled,r_selected)\n",
    "    r = [min(labeled_r)-0]\n",
    "\n",
    "    l=0\n",
    "    f1=[]\n",
    "    y_c_pred , y_prob , y_c_uncertainty = c_prediction(x_labeled, y_labeled, unlabeled_feature,model)\n",
    "    y_c = y_ground.copy()\n",
    "    y_c[unlabeled_index] = y_c_pred\n",
    "    f1.append(accuracy_score(y_ground, y_c))\n",
    "    \n",
    "    # Here choose min (r)>0 and len (r) < 50 if run the optimization objective \n",
    "    #while min(r) >0 and len(r) <50:\n",
    "    \n",
    "    # Change the maximum number of iteration based on your setting\n",
    "    \n",
    "    while l<20:\n",
    "        selection_index = random.sample(unlabeled_index,batch_size)\n",
    "        labeled_index= labeled_index+selection_index \n",
    "        x_labeled = X_feature[labeled_index]\n",
    "        unlabeled_index = remove_label(full_index,labeled_index)\n",
    "        unlabeled_feature = X_feature[unlabeled_index]\n",
    "        y_labeled = y_ground[labeled_index]\n",
    "        r_selected = y_r[labeled_index]\n",
    "        y_r_labeled = y_r[labeled_index]\n",
    "        labeled_feasible_sample, labeled_infeasible_sample,labeled_r = labeled_fea_sample(x_labeled,y_labeled,r_selected)\n",
    "        y_c_pred , y_prob , y_c_uncertainty = c_prediction(x_labeled, y_labeled, unlabeled_feature,model)\n",
    "        y_c = y_ground.copy()\n",
    "        y_c[unlabeled_index] = y_c_pred\n",
    "        f1.append(accuracy_score(y_ground, y_c))\n",
    "        r.append(min(labeled_r)-0)\n",
    "        l=l+1\n",
    "    result.append(r)\n",
    "    acc.append(f1)\n",
    "    print('Round {} finished'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "temp = np.array(list(zip_longest(*result, fillvalue=0))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimization performance\n",
    "opt_mean = temp.mean(axis=0)\n",
    "opt_std = temp.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#active learning performance\n",
    "mean_al=np.array(acc).mean(axis=0)\n",
    "std_al = np.array(acc).std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.74584289, 0.74881731, 0.74810912, 0.75261324, 0.75672077,\n",
       "       0.75842044, 0.75839211, 0.75921362, 0.75921362, 0.76210306,\n",
       "       0.76555905, 0.76960993, 0.77317923, 0.77587037, 0.78074276,\n",
       "       0.78094105, 0.78230078, 0.78402878, 0.78722982, 0.78921277,\n",
       "       0.78949605])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03487051, 0.03416555, 0.03604146, 0.03441147, 0.03341678,\n",
       "       0.03234378, 0.02996057, 0.03062733, 0.03100792, 0.03309726,\n",
       "       0.03193428, 0.03166732, 0.0322808 , 0.02989729, 0.02848445,\n",
       "       0.02686116, 0.02602347, 0.02544392, 0.02550865, 0.02569296,\n",
       "       0.02545505])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_al"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
